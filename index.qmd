---
title: "AB Testing is an Optimization Problem"
subtitle: A Bayesian Criterion for Stopping Experiments
author: "Russ Zaliznyak"
date: "2025-08-10"
execute:
  echo: false
format: 
  html: 
    toc: true
    toc-expand: true
    toc-indent: 1em
---

# Introduction

Significance testing, as traditionally applied in AB experiments, is often misaligned with business goals. While these statistical methods aim to minimize false positives, real business impact is frequently driven by avoiding false negatives — missed opportunities to create value.

Below I show why significance thresholds like α = 0.05 are arbitrary and harm decision-making. I introduce Bayesian testing as a more practical alternative, and demonstrate through simulation how it better supports value-driven experimentation.

You'll see that focusing on the expected value of our decisions is a far more profitable strategy than chasing statistical significance.

# Significance Testing

## Misaligned Goals

**Significance Testing**  is designed to rigorously control the frequency of false positives. If you're writing physics papers or claiming groundbreaking scientific discoveries, it's the perfect tool.

In these fields, false positives are extremely costly — both reputationally and scientifically. That’s why physicists often require 5-sigma evidence — about a 1 in 3.5 million chance that the result is a fluke.

But significance testing isn't designed to answer the more important business question:

Which decision has the highest expected return — ending the test, running it longer, or taking action now?

## Misaligned Costs

Most companies default to a significance threshold of **α = 0.05** and statistical power of **1 – β = 0.80**.
In practical terms, that means the test is designed to avoid **false positives** four times more aggressively than **false negatives**.

But in most business settings, **false positives aren’t that costly**. We thought a new button color improved conversion — turns out it didn’t. **So what?** The variant was flat, not disastrous.

But missing a true winner? That’s a lost opportunity — and sometimes, **a costly one**.



# Bayesian Testing

## Aligned Goals

The moment you ask, “What’s the expected value of this decision?”, you’ve entered Bayesian territory. You stop obsessing over p-values and start focusing on the projected impact of your actions.

Bayesian testing isn’t a new set of formulas — it’s a mindset shift. One that aligns probabilistic thinking with how businesses actually need to make decisions.


## Aligned Costs

We use expected loss as our stopping criterion — a measure that quantifies the risk of making the wrong decision, combining both the likelihood of harm and its expected magnitude:

$$
\text{Expected Loss} = \Pr(\text{harm}) \times \mathbb{E}[\text{Magnitude of Harm}]
$$


Rather than ask, “Is this statistically significant?”, we ask, “What’s the cost of making the wrong call?” This cost is expressed in the same units as the metric itself — for conversion rate, in percentage points.

We’ll refer to expected loss as `Risk`, and we’ll search for the optimal level of Risk to bear for any given experiment design — seeking to balance caution with speed to maximize long-term value.

# A Simulation Framework
```{python}
NUMBER_SIMULATIONS = int(5e4)
```

To evaluate these ideas under real-world constraints — limited time, finite traffic, and the tradeoff between speed and confidence — I simulate `{python} f"{NUMBER_SIMULATIONS:,}"` randomized AB tests with equal allocation to control and treatment.

The key advantage of this framework is that we know the true effect behind each experiment, while the analysis methods (Bayesian and Significance) operate blindly, vulnerable to noise and randomness. Only after all decisions are made do we compare their outcomes — measuring accuracy, timing, and business value.

## Experiment Design

Each of the `{python} f"{NUMBER_SIMULATIONS:,}"` experiments runs for up to 20 days, with traffic split 50/50 between control and treatment.
Users are randomly assigned daily, and outcomes are sampled using a binomial process — introducing natural variation that mimics how real experiments unfold over time.


| Min (Test Days) | Max (Test Days) | Max (Test N) | Post Test N | Conversion Rate |
|-----------------|------------------|---------------|---------------|------------------|
| 7               | 20               | 100,000        | 500,000       | 65%              |


Although the **Control** group has a true conversion rate of 65%, observed results will fluctuate due to random chance — just as they would in a real-world test.

## Expected Effects

Each experiment draws a treatment effect from a normal distribution centered at 100 with a standard error of 0.5 — modeling the natural variability seen in real-world tests, where small effects are common and large lifts are rare.

Even when a treatment is truly better (i.e., ITC > 100), it can underperform in a given simulation due to random variation — just as it might in practice.

```{python}
#| echo: false
#| code-fold: true
#| fig-cap: "Normal Distribution with μ = 100 and σ = 0.5."
from numpy.random import laplace, normal
from utilities.distributions import draw_distribution_classic
INTUIT_RED = "#bd0707"
INTUIT_BLUE = "#0177c9"

distribution_type = "normal"
#distribution_type = "laplace"
true_effect_mean = 100
true_effect_se = 0.5
distributions = {"laplace": laplace, "normal": normal}
dist_func = distributions.get(distribution_type, normal)  # fallback to normal
itc_samples = (
    dist_func(
        true_effect_mean, true_effect_se, int(1e5)
    )
    / 100
)

itc_fig = draw_distribution_classic(
  itc_samples * 100,
  is_rate=False,
  x_axis_title="Index-to-Control",
  colors=[INTUIT_RED, INTUIT_BLUE],
  add_median=True,
  title=f"{distribution_type.title()}",
  positive_direction=True,
)

itc_fig.show()

```

> Later We can swap in other distributions later, like Laplace.

## Candidate Risk Thresholds

A risk threshold defines how much expected loss we’re willing to tolerate before stopping the test and choosing a winner.
To explore tradeoffs between speed and precision, we simulate decisions across a range of candidate thresholds — eg (from 0.01% to 0.10%).
Lower thresholds are more conservative, delaying action until the risk is minimal. Higher thresholds allow faster decisions but accept more uncertainty.


## Simulation Logic

1. For each simulation, and each day after `Min (Test Days)`:
   - Compute **Risk (expected loss)** for both control and treatment conditions.
   - Compare each condition’s **Risk** to the candidate risk threshold.

2. Decision rules:
   - If **only one** condition breaches the threshold → select it as the winner.
   - If **both** breach → choose the condition with the **lower risk**.
   - If **neither** breaches by the end of the test → select the condition with the **lowest final risk**.

3. After selecting a winner:
   - Simulate rolling out the winner to the **remaining test population** and the **post-test population**, regardless of whether it was truly best.


<!--
::: {.callout-tip collapse="true" icon="" title="Example: Using Expected Loss as a Stopping Rule"}

- **Minimum test duration:** 7 days  
- **Current day:** 9  
- **Expected loss for Control:** 0.06 percentage points  
- **Expected loss for Treatment:** 0.02 percentage points  
- **Risk threshold:** 0.03 percentage points  


#### Decision Logic

- Treatment’s expected loss **falls below** the risk threshold → it's safe to stop and declare it the winner.

We stop the test **not because a p-value is below 0.05**, but because:

> “The cost of making the wrong decision is now acceptably low.”

This decision rule helps prioritize speed and safety without rigid statistical cutoffs.


:::

-->

This process lets me estimate how each candidate risk threshold shapes **decision quality** and the resulting **business value across the test and post-test populations**.

# Results


```{python}
from numpy import mean, abs, mean, minimum, maximum, array


def expected_loss(control_samples, treatment_samples, risk_type="loss"):
    delta = treatment_samples - control_samples
    true_delta = mean(treatment_samples) - mean(control_samples)

    if risk_type == "loss":
        expected_loss = abs(mean(minimum(delta, 0)))
        expected_gain = true_delta + expected_loss
    else:
        expected_loss = abs(mean(maximum(delta, 0)))
        expected_gain = expected_loss - true_delta

    return expected_gain, expected_loss


```


```{python}
#| echo: false
#| code-fold: true
import sys
utilities_path = './utilities/'
sys.path.insert(0, utilities_path)

import experimentSimulationObject as eso
import power_methods as pm

from numpy import sum, array, argmax, argmin
from numpy.random import normal
from math import ceil
from scipy.stats import norm
from timeit import default_timer
import plotly.graph_objects as go

# --- Constants & Parameters ---
alpha = 0.05
power = 0.80

control_mean = 0.65
control_std = (control_mean * (1 - control_mean)) ** 0.5
is_rate = True
number_conditions = 2
allocation_list = array([0.5, 0.5])
min_test_days = 7
max_test_days = 20
max_test_n = int(1e5)
post_test_n = int(5e5)
metric_direction = "increase"
risk_type = "loss" if metric_direction.lower() == "increase" else "gain"
index_to_control_planned = 1.01
treatment_mean = control_mean * index_to_control_planned

control_trials, treatment_trials = tuple(allocation_list * max_test_n)
number_daily_visitors = max_test_n / max_test_days
control_se = control_std / control_trials**0.5
treatment_se = control_se

z_critical = (
    norm.ppf(1 - alpha) if metric_direction == "increase" else norm.ppf(alpha)
)

# --- Sample Size Calculations ---
fixed_sample_limit = sum(pm.two_proportion_required_samples(
    alpha,
    power,
    control_mean,
    treatment_mean,
    control_to_treatment_ratio=1,
    tail_type="one_tail",
))

max_fixed_volume = (
    fixed_sample_limit if max_test_n >= fixed_sample_limit else max_test_n
)

# Ensure fixed step respects bounds
actual_fixed_days = min(
    ceil(max_fixed_volume / number_daily_visitors), max_test_days
)
actual_fixed_days = max(actual_fixed_days, min_test_days)

max_volume_year = post_test_n + max_test_n

# --- Simulate Control/Treatment Samples ---
control_samples = normal(control_mean, control_se, 10000)
treatment_samples = normal(treatment_mean, treatment_se, 10000)

e_gain, e_loss = expected_loss(
    control_samples, treatment_samples, risk_type=risk_type
)
max_threshold = 0.5 * max(e_gain, e_loss)

n_increments = 120
step = max_threshold / (n_increments if n_increments > 0 else 0)
threshold_values = [round(i * step, 10) for i in range(n_increments + 1)]


all_simulations = []
start = default_timer()

for j in range(NUMBER_SIMULATIONS):
    a_simulation = eso.experimentSimulationObject(
        control_mean,
        control_std,
        itc_samples[j],
        is_rate,
        number_conditions,
        allocation_list,
        min_test_days,
        max_test_days,
        max_test_n,
        max_volume_year,
    )

    a_simulation.calculate_expected_loss(risk_type)
    a_simulation.update_potential_outcomes_at_each_test_day()
    a_simulation.calculate_optimal_stopping_values(threshold_values, risk_type)
    a_simulation.calculate_llr(index_to_control_planned)
    a_simulation.calculate_sequential_stopping_values(alpha, power)
    a_simulation.calculate_z_scores(actual_fixed_days=actual_fixed_days)

    a_simulation.calculate_fixed_stopping_values(
        z_critical=z_critical,
        test_direction=metric_direction,
        actual_fixed_days=actual_fixed_days,
    )
    all_simulations.append(a_simulation)


bayesian_analysis_dictionary_results = (
            eso.analyze_experiment_simulations_bayesian(
                all_simulations, metric_direction
            )
        )
sequential_analysis_dictionary_results = (
    eso.analyze_experiment_simulations_sequential(all_simulations)
)
fixed_analysis_dictionary_results = eso.analyze_experiment_simulations_fixed(
    all_simulations
)

stop = default_timer()
#print(len(bayesian_analysis_dictionary_results["all_means"]))
#print(len(threshold_values))
#print(f"simulation run-time: {stop-start:.2f} secs")


potentials = bayesian_analysis_dictionary_results["all_means"]
times = bayesian_analysis_dictionary_results["run_times"]
winners = bayesian_analysis_dictionary_results["winner_rate"]

sprt_potentials = sequential_analysis_dictionary_results["sequential_potential_mean"],
sprt_winners = sequential_analysis_dictionary_results["sequential_winner_rate"],
sprt_times = sequential_analysis_dictionary_results["sequential_run_times"],
fixed_potentials = fixed_analysis_dictionary_results["fixed_potential_mean"],
fixed_winners = fixed_analysis_dictionary_results["fixed_winner_rate"],
fixed_times = actual_fixed_days


def format_potential(p, is_rate: bool) -> str:
    return f"{p:.3%}" if is_rate else f"{p:.3f}"
figure_text = [
            f"Risk Threshold: {format_potential(v, is_rate)} <br>"
            f"Projected Mean: {format_potential(p, is_rate)} <br>"
            f"Expected # of Trials: {int(t):,} <br>"
            f"Selected Best Condition: {w:.2%}"
            for v, 
              t,
               w,
                p 
                in zip(threshold_values, 
            times, 
            winners, 
            potentials
            )
        ]



threshold_figure = go.Figure()
threshold_figure.add_trace(
    go.Scatter(
        x=threshold_values,
        y=potentials,
        text=figure_text,
        mode="lines",
        showlegend=False,
        line_shape="spline",
        hoverlabel=dict(font=dict(size=21)),
        line=dict(width=2),
        hoverinfo="text",
    )
)


if metric_direction == "increase":
  optimal_index = argmax(potentials)

else:
  optimal_index = argmin(potentials)
optimal_risk_level = threshold_values[optimal_index]
optimal_index = argmin(abs(array(threshold_values) - optimal_risk_level))


threshold_figure.add_trace(
  go.Scatter(
      x=[optimal_risk_level],
      y=[potentials[optimal_index]],
      mode="markers",
      showlegend = False,
      marker=dict(
    size=16,
    color="#2ECC71",  # soft green
    symbol="x"
)
,
      # name=f"Experiment {'Maximum' if metric_direction == 'increase' else 'Minimum'}",
      name="Optimal Threshold",
      hoverinfo="skip",
  )
)



tickformat = ".3%" if is_rate else ".3f"
threshold_figure.update_layout(
    title=dict(
        text="<b>Projected Mean by Risk Threshold</b>",
        font=dict(size=24),
        x=0.5,
        xanchor="center",
    ),
    margin=dict(l=110, r=50, b=55, t=45, pad=1),
    xaxis=dict(
        # type="log",
        tickformat=tickformat,
        #tickvals=tickvals.tolist(),
        #ticktext=ticktext,
        title="Risk Threshold",
        title_font=dict(size=21),
        tickfont=dict(size=16),
    ),
    yaxis=dict(
        tickformat=tickformat,
        title="Projected Mean @ Year-End",
        title_font=dict(size=21),
        tickfont=dict(size=16),
    ),
    showlegend=True,
)

pass

```


## Optimization Curve

With Bayesian testing, I don’t get an arbitrary threshold — I get a curve. It maps projected year-end value across the test and post-test populations for each candidate risk threshold. This lets me see how different levels of risk tolerance actually perform — and choose the one that maximizes long-term business value. 

```{python}
#| echo: false
#| code-fold: true
#| fig-cap: "❎ represents the Optimal Risk Threshold because it maximizes expected # of conversions @ year-end"
threshold_figure.show()

```

## Insights

1. Lower thresholds are more cautious; higher ones accept more risk to enable faster decisions. Hover over the curve to see how `Expected # of trials` decreases with higher thresholds. <br><br>
    - You can choose to run a faster experiment, but you'll have to sacrifice some expected value.  <br><br>
2. Bayesian Analysis yields 50% `Selected Best Condition` rate because the mean of treatment effects is centered at 100, regardless of Risk Threshold.
    - Sneak Peek: Do we expect better or worse accuracy from Significance Tests? <span style="font-size: 2em;">😉</span>

## Bayes Vs. Significance

Let’s see how the same `{python} f"{NUMBER_SIMULATIONS:,}"` simulations perform under traditional significance testing. We’ll use a standard setup: α = 0.05, 80% power, and a detectable effect of 101.

How does this legacy approach stack up against Bayesian testing in practice?

### Comparison
```{python}
#| echo: false
#| output: true
import pandas as pd
from IPython.display import display

# Build the core DataFrame
import pandas as pd
from IPython.display import display

# Build initial DataFrame in arbitrary order (we’ll reorder later)
df = pd.DataFrame({
    "Metric": [
        "Projected Mean @ Year-End",
        "Selected Best Condition",
        "Mean Run Time"
    ],
    "Bayesian": [
        potentials[optimal_index],
        winners[optimal_index],
        times[optimal_index] / number_daily_visitors
    ],
    "SPRT": [
        sequential_analysis_dictionary_results["sequential_potential_mean"],
        sequential_analysis_dictionary_results["sequential_winner_rate"],
        sequential_analysis_dictionary_results["sequential_run_times"]
    ],
    "Fixed": [
        fixed_analysis_dictionary_results["fixed_potential_mean"],
        fixed_analysis_dictionary_results["fixed_winner_rate"],
        actual_fixed_days
    ]
})

# Compute Marginal Events
sprt_marginal = max_volume_year * (
    sequential_analysis_dictionary_results["sequential_potential_mean"] - control_mean
)
fixed_marginal = max_volume_year * (
    fixed_analysis_dictionary_results["fixed_potential_mean"] - control_mean
)
bayes_marginal = max_volume_year * (
    potentials[optimal_index] - control_mean
)

# Append marginal row
df.loc[len(df.index)] = [
    "Marginal Events vs No Test",
    bayes_marginal,
    sprt_marginal,
    fixed_marginal
]

# Reorder rows to: Selected Best, Marginal, Projected Mean, Run Time
row_order = [
    "Selected Best Condition",
    "Marginal Events vs No Test",
    "Projected Mean @ Year-End",
    "Mean Run Time"
]
df = df.set_index("Metric").loc[row_order].reset_index()

# Format based on row type
formatted_df = df.copy()

for idx, row in df.iterrows():
    metric = row["Metric"]
    if metric in ["Projected Mean @ Year-End", "Selected Best Condition"]:
        formatted_df.at[idx, "Bayesian"] = f"{row['Bayesian']:.2%}"
        formatted_df.at[idx, "SPRT"] = f"{row['SPRT']:.2%}"
        formatted_df.at[idx, "Fixed"] = f"{row['Fixed']:.2%}"
    elif metric == "Marginal Events vs No Test":
        formatted_df.at[idx, "Bayesian"] = f"{row['Bayesian']:,.0f}"
        formatted_df.at[idx, "SPRT"] = f"{row['SPRT']:,.0f}"
        formatted_df.at[idx, "Fixed"] = f"{row['Fixed']:,.0f}"
    else:
        bayes_raw = row["Bayesian"] * number_daily_visitors
        sprt_raw = row["SPRT"] * number_daily_visitors
        fixed_raw = row["Fixed"] * number_daily_visitors

        formatted_df.at[idx, "Bayesian"] = f"N = {bayes_raw:,.0f} ({row['Bayesian']:.2f} days)"
        formatted_df.at[idx, "SPRT"] = f"N = {sprt_raw:,.0f} ({row['SPRT']:.2f} days)"
        formatted_df.at[idx, "Fixed"] = f"N = {fixed_raw:,.0f} ({row['Fixed']:,.0f} days)"



# Display without index
display(formatted_df.style.hide(axis="index"))



```
<br>

`Marginal Events Vs No Test`: The additional number of conversions we expect to gain by running the experiment, compared to doing nothing at all.

`Projected Mean @ Year-End`: The average performance you'd expect if you ran this test and deployed the winning variant — including both the test period and the post-test rollout; reflects thet total impact of the experiment.

> Marginal Events vs No Test is directly derived from the Projected Mean @ Year-End. It quantifies the gain by multiplying the projected lift over control by the total number of users — turning a small percentage improvement into a tangible number of additional conversions.

## Summary

Significance Testing, SPRT or Fixed, perform terribly in a world where treatment effects are centered at 100.
**Why?** These were designed to detect effects at 101, but the mean of true effects is 100. As a result, most of the time **significance testing** will leave these small marginal gains on the table. 


# Glossary

::: {.callout-tip collapse="true" icon="" title="Significance Testing Glossary"}

### α (Alpha)
- **What it is**: The *significance level* — the probability of a **false positive** (Type I error).
- **Typical value**: `0.05` (or 5%)
- **In context**: If `α = 0.05`, there’s a 5% chance you’ll wrongly conclude a difference exists when it doesn’t.

---

### β (Beta)
- **What it is**: The probability of a **false negative** (Type II error).
- **Power = 1 – β**: A common target is `80%` power, meaning a `20%` chance of missing a true effect.
- **In context**: A high β means you risk overlooking a variant that’s actually better — lost upside.

---

### SPRT (Sequential Probability Ratio Test)
- **What it is**: A method that checks for significance **continuously** during the test.
- **Benefit**: Can stop experiments early if strong evidence accumulates.
- **In context**: Faster than fixed-duration tests, but may be overly conservative depending on thresholds.

---

### Mean Detectable Effect
- **What it is**: The *smallest effect size* you expect to detect reliably in your test.
- **In context**: If you set this to `1%`, your test is designed to detect changes that are at least that big — smaller changes might go unnoticed.

:::