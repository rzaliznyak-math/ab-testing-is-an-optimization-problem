---
title: "AB Testing is An Optimization Problem"
subtitle: My Journey to Bayesian Testing
author: "Russ Zaliznyak"
date: "2025-08-05"
execute:
  echo: false
format: 
  html: 
    toc: true
    toc-expand: true
    toc-indent: 1em
---

# Introduction

I first heard about "AB Testing" way back in 2008 when I joined the tech world. My bachelor's from San Jose State in mathematics exposed me to hypothesis testing. So p-values, statistical power, and confidence intervals were all familiar concepts to me. 

Over the years I built many iterations of experiment design and analysis tools. I used the CLT to expand tools to support continuous metrics like revenue and time-on-site. Later I discovered that the Sequential Probability Ratio Test could help me to run faster experiments. And I wrote numpy simulations to estimate SPRT runtime in our design tools.

I built apps to design and analyze sequential significance experiments. I became an expert on controlling the frequency of Type I errors (false positives) and Type II errors (false negatives). 

**But so what?** 

Are these frequentist methods generating value for the company? Or is it an exercise in pedantry?
Within a few short paragraphs, you'll see why significance testing in industry is massively misused.

# Significance Testing

## Misaligned Goals

**Significance Testing**  is designed to rigorously control the frequency of false positives. If you're writing physics papers or claiming groundbreaking scientific discoveries, it's the perfect tool.

In these fields, false positives are extremely costly — both reputationally and scientifically. That’s why physicists often require 5-sigma evidence — about a 1 in 3.5 million chance that the result is a fluke.

But significance testing says nothing about the most important question:

Which decision has the highest expected return — ending the test, running it longer, or taking action now?

## Misaligned Costs

Most companies default to a significance threshold of **α = 0.05** and statistical power of **1 – β = 0.80**.
In practical terms, that means the test is designed to avoid **false positives** four times more aggressively than **false negatives**.

But in most business settings, **false positives aren’t that costly**. We thought a new button color improved conversion — turns out it didn’t. **So what?** The variant was flat, not disastrous.

But missing a true winner? That’s a lost opportunity — and sometimes, **a costly one**.

What's the right balance to take? What are the factors that affect the balance?

# Bayesian Testing

## Aligning Goals

The moment you ask, “What’s the expected value of this decision?”, you’ve entered Bayesian territory. You stop obsessing over p-values and start focusing on the projected impact of your actions.

Bayesian testing isn’t a new set of formulas — it’s a philosophical shift. One that aligns statistical thinking with how businesses actually need to make decisions.




